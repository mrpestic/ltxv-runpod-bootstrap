#!/usr/bin/env python3
"""
–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π inference –¥–µ–º–æ–Ω –¥–ª—è LTX-Video
–ó–∞–ø—É—Å–∫–∞—Ç—å: python inference_daemon_official.py
"""

import os
import sys
import json
import time
import glob
import subprocess
import torch
import logging
from pathlib import Path

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('inference_daemon_official.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ inference –Ω–∞–ø—Ä—è–º—É—é
from ltx_video.inference import infer, InferenceConfig, load_pipeline_config, create_ltx_video_pipeline, get_device, calculate_padding, get_unique_filename, seed_everething
from ltx_video.pipelines.pipeline_ltx_video import SkipLayerStrategy

def create_ready_flag():
    """–°–æ–∑–¥–∞–µ–º —Ñ–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –¥–µ–º–æ–Ω–∞"""
    with open("daemon_ready.flag", "w") as f:
        f.write(str(time.time()))

def clear_gpu_cache():
    """–û—á–∏—â–∞–µ–º GPU –∫–µ—à, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏"""
    if torch.cuda.is_available():
        # –û—á–∏—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –Ω–µ –º–æ–¥–µ–ª–∏
        torch.cuda.empty_cache()
        logger.info("üßπ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è pipeline
global_pipeline = None
global_pipeline_config = None

def load_models_once():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑ –∏ –¥–µ—Ä–∂–∏–º –≤ –ø–∞–º—è—Ç–∏"""
    global global_pipeline, global_pipeline_config
    
    logger.info("üé¨ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –≤ GPU...")
    
    try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
        global_pipeline_config = load_pipeline_config("ltxv-13b-0.9.8-distilled.yaml")
        logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω")
        

        # –°–æ–∑–¥–∞–µ–º pipeline –æ–¥–∏–Ω —Ä–∞–∑
        logger.info("üéØ –°–æ–∑–¥–∞–µ–º pipeline...")
        global_pipeline = create_ltx_video_pipeline(
            ckpt_path=global_pipeline_config["checkpoint_path"],
            precision=global_pipeline_config["precision"],
            text_encoder_model_name_or_path=global_pipeline_config["text_encoder_model_name_or_path"],
            enhance_prompt=True,
            prompt_enhancer_image_caption_model_name_or_path=global_pipeline_config.get(
                "prompt_enhancer_image_caption_model_name_or_path"
            ),
            prompt_enhancer_llm_model_name_or_path=global_pipeline_config.get(
                "prompt_enhancer_llm_model_name_or_path"
            ),
        )
        
        # –ï—Å–ª–∏ —ç—Ç–æ multi-scale pipeline, —Å–æ–∑–¥–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π wrapper
        if global_pipeline_config.get("pipeline_type") == "multi-scale":
            from ltx_video.pipelines.pipeline_ltx_video import LTXMultiScalePipeline
            from ltx_video.inference import create_latent_upsampler
            
            spatial_upscaler_model_path = global_pipeline_config.get("spatial_upscaler_model_path")
            if spatial_upscaler_model_path:
                logger.info("üéØ –°–æ–∑–¥–∞–µ–º latent upsampler...")
                latent_upsampler = create_latent_upsampler(spatial_upscaler_model_path, global_pipeline.device)
                global_pipeline = LTXMultiScalePipeline(global_pipeline, latent_upsampler=latent_upsampler)
                logger.info("‚úÖ Multi-scale pipeline —Å–æ–∑–¥–∞–Ω")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º pipeline –Ω–∞ GPU
        device = get_device()
        logger.info(f"üéØ –ü–µ—Ä–µ–º–µ—â–∞–µ–º pipeline –Ω–∞ {device}...")
        if hasattr(global_pipeline, 'video_pipeline'):
            # –≠—Ç–æ multi-scale pipeline
            global_pipeline.video_pipeline = global_pipeline.video_pipeline.to(device)
            # –¢–∞–∫–∂–µ –ø–µ—Ä–µ–º–µ—â–∞–µ–º latent_upsampler –Ω–∞ GPU
            if hasattr(global_pipeline, 'latent_upsampler'):
                global_pipeline.latent_upsampler = global_pipeline.latent_upsampler.to(device)
                logger.info(f"üéØ Latent upsampler –ø–µ—Ä–µ–º–µ—â–µ–Ω –Ω–∞ {device}")
        else:
            # –û–±—ã—á–Ω—ã–π pipeline
            global_pipeline = global_pipeline.to(device)
        
        logger.info("‚úÖ Pipeline —Å–æ–∑–¥–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
        return True
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        return False

def test_pipeline():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º pipeline –ø—Ä–æ—Å—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π"""
    global global_pipeline
    
    if global_pipeline is None:
        logger.error("‚ùå Pipeline –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        return False
    
    try:
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º pipeline...")
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        test_config = InferenceConfig(
            prompt="test",
            negative_prompt="worst quality",
            height=512,
            width=512,
            num_frames=8,
            seed=42,
            pipeline_config="ltxv-13b-0.9.8-distilled.yaml"
        )
        
        result = generate_with_pipeline(test_config, global_pipeline, global_pipeline_config)
        logger.info(f"‚úÖ –¢–µ—Å—Ç —É—Å–ø–µ—à–µ–Ω: {result}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –¢–µ—Å—Ç pipeline –Ω–µ –ø—Ä–æ—à—ë–ª: {e}")
        return False

def generate_with_pipeline(config, pipeline, pipeline_config):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –∏—Å–ø–æ–ª—å–∑—É—è –≥–æ—Ç–æ–≤—ã–π pipeline"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º subprocess –¥–ª—è –≤—ã–∑–æ–≤–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ inference.py
    # –ù–æ –ù–ï –æ—á–∏—â–∞–µ–º GPU –∫–µ—à –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—è–º–∏
    import subprocess
    import glob
    import os
    from datetime import datetime
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è inference.py
    cmd = [
        "/workspace/LTX-Video/env/bin/python", "inference.py",
        "--prompt", config.prompt,
        "--negative_prompt", config.negative_prompt,
        "--height", str(config.height),
        "--width", str(config.width),
        "--num_frames", str(config.num_frames),
        "--seed", str(config.seed),
        "--pipeline_config", "ltxv-13b-0.9.8-distilled.yaml"
    ]
    
    if config.conditioning_media_paths:
        cmd.extend(["--conditioning_media_paths", config.conditioning_media_paths[0]])
        cmd.extend(["--conditioning_start_frames", "0"])
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º inference.py —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    logger.info(f"üéØ –ó–∞–ø—É—Å–∫–∞–µ–º: {' '.join(cmd)}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º subprocess —Å –≤—ã–≤–æ–¥–æ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    process = subprocess.Popen(
        cmd, 
        stdout=subprocess.PIPE, 
        stderr=subprocess.STDOUT, 
        text=True, 
        cwd="/workspace/LTX-Video",
        bufsize=1,
        universal_newlines=True
    )
    
    # –ß–∏—Ç–∞–µ–º –≤—ã–≤–æ–¥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    output_lines = []
    while True:
        output = process.stdout.readline()
        if output == '' and process.poll() is not None:
            break
        if output:
            output = output.strip()
            logger.info(f"üîß inference.py: {output}")
            output_lines.append(output)
    
    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
    returncode = process.poll()
    
    if returncode != 0:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ inference.py (–∫–æ–¥: {returncode})")
        return []
    
    # –ò—â–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ outputs
    output_files = []
    
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ stdout
    for line in output_lines:
        if "Output saved to" in line:
            output_path = line.split("Output saved to")[1].strip()
            if os.path.exists(output_path):
                output_files.append(output_path)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {output_path}")
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ stdout, –∏—â–µ–º –≤ –ø–∞–ø–∫–µ outputs
    if not output_files:
        output_dir = f"outputs/{datetime.today().strftime('%Y-%m-%d')}"
        if os.path.exists(output_dir):
            mp4_files = glob.glob(f"{output_dir}/*.mp4")
            if mp4_files:
                # –ë–µ—Ä—ë–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
                latest_file = max(mp4_files, key=os.path.getctime)
                output_files.append(latest_file)
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {latest_file}")
    
    return output_files

def infer_with_ready_pipeline(config, ready_pipeline, pipeline_config):
    """–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è infer() –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–π pipeline"""
    import torch
    import numpy as np
    import imageio
    from datetime import datetime
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    device = get_device()
    seed_everething(config.seed)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
    height_padded = ((config.height - 1) // 32 + 1) * 32
    width_padded = ((config.width - 1) // 32 + 1) * 32
    num_frames_padded = ((config.num_frames - 2) // 8 + 1) * 8 + 1
    
    padding = calculate_padding(config.height, config.width, height_padded, width_padded)
    
    logger.warning(f"Padded dimensions: {height_padded}x{width_padded}x{num_frames_padded}")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    sample = {
        "prompt": config.prompt,
        "prompt_attention_mask": None,
        "negative_prompt": config.negative_prompt,
        "negative_prompt_attention_mask": None,
    }
    
    # üî• –ö–†–ò–¢–ò–ß–ù–û: generator –Ω–∞ CPU —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–∂–∞–ª –ø–∞–º—è—Ç—å –Ω–∞ GPU
    generator = torch.Generator(device="cpu").manual_seed(config.seed)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ STG
    stg_mode = pipeline_config.get("stg_mode", "attention_values")
    if stg_mode.lower() == "stg_av" or stg_mode.lower() == "attention_values":
        skip_layer_strategy = SkipLayerStrategy.AttentionValues
    elif stg_mode.lower() == "stg_as" or stg_mode.lower() == "attention_skip":
        skip_layer_strategy = SkipLayerStrategy.AttentionSkip
    elif stg_mode.lower() == "stg_r" or stg_mode.lower() == "residual":
        skip_layer_strategy = SkipLayerStrategy.Residual
    elif stg_mode.lower() == "stg_t" or stg_mode.lower() == "transformer_block":
        skip_layer_strategy = SkipLayerStrategy.TransformerBlock
    else:
        raise ValueError(f"Invalid spatiotemporal guidance mode: {stg_mode}")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º conditioning –µ—Å–ª–∏ –µ—Å—Ç—å
    media_item = None
    conditioning_items = None
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º conditioning_media_paths (–¥–ª—è image-to-video –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ)
    if config.conditioning_media_paths:
        from ltx_video.inference import prepare_conditioning
        logger.info(f"üñºÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º conditioning images: {config.conditioning_media_paths}")
        conditioning_items = prepare_conditioning(
            conditioning_media_paths=config.conditioning_media_paths,
            conditioning_strengths=config.conditioning_strengths or [1.0],
            conditioning_start_frames=config.conditioning_start_frames or [0],
            height=config.height,
            width=config.width,
            num_frames=config.num_frames,
            padding=padding,
            pipeline=ready_pipeline,
        )
        logger.info(f"üñºÔ∏è Conditioning items: {type(conditioning_items)} {len(conditioning_items) if conditioning_items else 'None'}")
        
    # input_media_path –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è image-to-video (—Ç–æ–ª—å–∫–æ –¥–ª—è video-to-video)
    if config.input_media_path:
        from ltx_video.inference import load_media_file
        logger.info(f"üñºÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º input media: {config.input_media_path}")
        media_item = load_media_file(
            media_path=config.input_media_path,
            height=config.height,
            width=config.width,
            max_frames=num_frames_padded,
            padding=padding,
        )
        logger.info(f"üñºÔ∏è Input media item shape: {media_item.shape if hasattr(media_item, 'shape') else type(media_item)}")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ timesteps –∫–∞–∫ –≤ inference.py - –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è image-to-video
    timesteps = pipeline_config.get("first_pass", {}).get("timesteps", [1.0, 0.9937, 0.9875, 0.9812, 0.975, 0.9094, 0.725])
    if conditioning_items is not None:
        logger.info(f"üñºÔ∏è Image-to-video: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–µ timesteps –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ: {timesteps}")
    else:
        logger.info(f"üìù Text-to-video: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–µ timesteps: {timesteps}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ multi-scale pipeline
    if hasattr(ready_pipeline, 'video_pipeline'):
        # Multi-scale pipeline - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–∞ –ø—Ä–æ—Ö–æ–¥–∞ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
        first_pass_config = pipeline_config.get("first_pass", {}).copy()
        logger.info(f"üé¨ Multi-scale: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ timesteps –≤ first_pass: {timesteps}")
        
        # üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            logger.info(f"üî• –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π: {allocated:.1f}GB allocated, {cached:.1f}GB cached")
            torch.cuda.empty_cache()
            logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ GPU –∫–µ—à–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (multi-scale)")
        
        # üî• –ö–†–ò–¢–ò–ß–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º no_grad –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è autograd (–ø–æ–∑–≤–æ–ª—è–µ—Ç callback'–∏)
        with torch.no_grad():
            images = ready_pipeline(
                downscale_factor=pipeline_config.get("downscale_factor", 0.6666666),
                first_pass=first_pass_config,
                second_pass=pipeline_config.get("second_pass", {}),
                skip_layer_strategy=skip_layer_strategy,
                generator=generator,
                output_type="pt",
                callback_on_step_end=None,
                height=height_padded,
                width=width_padded,
                num_frames=num_frames_padded,
                frame_rate=config.frame_rate,
                **sample,
                media_items=media_item,
                conditioning_items=conditioning_items,
                is_video=True,
                vae_per_channel_normalize=True,
                image_cond_noise_scale=config.image_cond_noise_scale,
                mixed_precision=(pipeline_config.get("precision") == "mixed_precision"),
                offload_to_cpu=False,
                device=device,
                enhance_prompt=True,
            ).images
        
        # üî• –ö–†–ò–¢–ò–ß–ù–û: –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ multi-scale
        if torch.cuda.is_available():
            allocated_before = torch.cuda.memory_allocated() / 1024**3
            cached_before = torch.cuda.memory_reserved() / 1024**3
            logger.info(f"üßπ –ü–∞–º—è—Ç—å –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ multi-scale: {allocated_before:.1f}GB allocated, {cached_before:.1f}GB cached")
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            
            allocated_after = torch.cuda.memory_allocated() / 1024**3
            cached_after = torch.cuda.memory_reserved() / 1024**3
            freed_allocated = allocated_before - allocated_after
            freed_cached = cached_before - cached_after
            logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ multi-scale: {allocated_after:.1f}GB allocated, {cached_after:.1f}GB cached")
            logger.info(f"üßπ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏: {freed_allocated:.1f}GB allocated, {freed_cached:.1f}GB cached")
        
        # üî• –û—á–∏—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ multi-scale
        del first_pass_config
        if 'generator' in locals():
            del generator
        
        # üî• –ö–†–ò–¢–ò–ß–ù–û: –æ—á–∏—â–∞–µ–º conditioning —Ç–µ–Ω–∑–æ—Ä—ã —Å GPU
        if conditioning_items is not None:
            try:
                # –ï—Å–ª–∏ —Ç–∞–º —Ç–µ–Ω–∑–æ—Ä—ã - –≤—ã–≥—Ä—É–∂–∞–µ–º –Ω–∞ CPU
                conditioning_items = [
                    (t.detach().to("cpu", copy=False) if torch.is_tensor(t) else t)
                    for t in conditioning_items
                ]
            except Exception:
                pass
        del conditioning_items, media_item
    else:
        # –û–±—ã—á–Ω—ã–π pipeline
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ GPU –∫–µ—à–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (single-scale)")
        # üî• –ö–†–ò–¢–ò–ß–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º no_grad –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è autograd (–ø–æ–∑–≤–æ–ª—è–µ—Ç callback'–∏)
        with torch.no_grad():
            images = ready_pipeline(
                skip_layer_strategy=skip_layer_strategy,
                generator=generator,
                output_type="pt",
                callback_on_step_end=None,
                height=height_padded,
                width=width_padded,
                num_frames=num_frames_padded,
                frame_rate=config.frame_rate,
                **sample,
                media_items=media_item,
                conditioning_items=conditioning_items,
                is_video=True,
                vae_per_channel_normalize=True,
                image_cond_noise_scale=config.image_cond_noise_scale,
                mixed_precision=(pipeline_config.get("precision") == "mixed_precision"),
                offload_to_cpu=False,
                device=device,
                enhance_prompt=True,
                timesteps=timesteps,
                guidance_scale=pipeline_config.get("first_pass", {}).get("guidance_scale", 1.0),
                stg_scale=pipeline_config.get("first_pass", {}).get("stg_scale", 0.0),
                rescaling_scale=pipeline_config.get("first_pass", {}).get("rescaling_scale", 1.0),
                skip_block_list=pipeline_config.get("first_pass", {}).get("skip_block_list", [42]),
            ).images
    
    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    (pad_left, pad_right, pad_top, pad_bottom) = padding
    pad_bottom = -pad_bottom
    pad_right = -pad_right
    if pad_bottom == 0:
        pad_bottom = images.shape[3]
    if pad_right == 0:
        pad_right = images.shape[4]
    images = images[:, :, : config.num_frames, pad_top:pad_bottom, pad_left:pad_right]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥–µ–æ
    output_dir = Path(config.output_path) if config.output_path else Path(f"outputs/{datetime.today().strftime('%Y-%m-%d')}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    result_paths = []
    for i in range(images.shape[0]):
        video_np = images[i].permute(1, 2, 3, 0).cpu().float().numpy()
        video_np = (video_np * 255).astype(np.uint8)
        
        output_filename = get_unique_filename(
            f"video_output_{i}",
            ".mp4",
            prompt=config.prompt,
            seed=config.seed,
            resolution=(config.height, config.width, config.num_frames),
            dir=output_dir,
        )
        
        imageio.mimsave(output_filename, video_np, fps=config.frame_rate)
        logger.info(f"Output saved to {output_filename}")
        result_paths.append(str(output_filename))
    
    # üî• –ö–†–ò–¢–ò–ß–ù–û: –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    if torch.cuda.is_available():
        # üî• –ê–ì–†–ï–°–°–ò–í–ù–û: –≤—ã–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ CPU –∏ —Å—Ä–∞–∑—É –æ—á–∏—â–∞–µ–º GPU
        images_cpu = images.detach().to("cpu", copy=True)  # –æ—Ç–¥–µ–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä –Ω–∞ CPU
        del images  # —É–¥–∞–ª—è–µ–º —Å GPU
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        logger.info(f"üßπ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {allocated:.1f}GB allocated, {cached:.1f}GB cached")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º images_cpu –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–¥–µ–æ
        images = images_cpu
    
    # üî• –§–ò–ù–ê–õ–¨–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª—è–µ–º –≤—Å–µ –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–∫—Ç—ã
    del images_cpu
    if 'conditioning_items' in locals():
        del conditioning_items
    if 'media_item' in locals():
        del media_item
    if 'generator' in locals():
        del generator
    import gc
    gc.collect()
    torch.cuda.synchronize()
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()  # –ø–æ–º–æ–≥–∞–µ—Ç –¥–æ–±–∏—Ç—å IPC-—Ö—ç–Ω–¥–ª—ã
    
    return result_paths

def process_command_file(command_file):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–∞–Ω–¥—É –∏–∑ —Ñ–∞–π–ª–∞"""
    global global_pipeline, global_pipeline_config
    
    try:
        # –ß–∏—Ç–∞–µ–º –∫–æ–º–∞–Ω–¥—É
        with open(command_file, 'r') as f:
            command = json.load(f)
        
        logger.info(f"üì• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {command_file}")
        logger.info(f"üé¨ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–¥–µ–æ: {command['prompt'][:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ pipeline –≥–æ—Ç–æ–≤
        if global_pipeline is None:
            raise Exception("Pipeline –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è inference
        inference_config = InferenceConfig(
            prompt=command['prompt'],
            negative_prompt=command['negative_prompt'],
            height=command['height'],
            width=command['width'],
            num_frames=command['num_frames'],
            seed=command['seed'],
            pipeline_config="ltxv-13b-0.9.8-distilled.yaml",
            frame_rate=24  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 24 FPS –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –≤–∏–¥–µ–æ
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if command.get('image_path') and os.path.exists(command['image_path']):
            # –î–ª—è image-to-video –∏—Å–ø–æ–ª—å–∑—É–µ–º conditioning_media_paths –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º —Å–∫—Ä–∏–ø—Ç–µ
            inference_config.conditioning_media_paths = [command['image_path']]
            inference_config.conditioning_start_frames = [0]
            logger.info(f"üñºÔ∏è –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º conditioning_media_paths: {command['image_path']}")
        
        logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π pipeline...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π pipeline –Ω–∞–ø—Ä—è–º—É—é (–±–µ–∑ subprocess)
        result_paths = infer_with_ready_pipeline(inference_config, global_pipeline, global_pipeline_config)
        
        # –ò—â–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ outputs
        output_dir = f"outputs/{time.strftime('%Y-%m-%d')}"
        if os.path.exists(output_dir):
            mp4_files = glob.glob(f"{output_dir}/*.mp4")
            if mp4_files:
                # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
                video_path = max(mp4_files, key=os.path.getctime)
                
                if os.path.exists(video_path):
                    logger.info(f"‚úÖ –í–∏–¥–µ–æ —Å–æ–∑–¥–∞–Ω–æ: {video_path}")
                    
                    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    result = {
                        'status': 'success',
                        'result': video_path,
                        'command_id': os.path.basename(command_file).replace('command_', '').replace('.json', '')
                    }
                    
                    return result
                else:
                    raise Exception(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")
            else:
                raise Exception("–ù–µ –Ω–∞–π–¥–µ–Ω—ã MP4 —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ outputs")
        else:
            raise Exception("–ü–∞–ø–∫–∞ outputs –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            
    except Exception as e:
        import traceback
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥—ã: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        return {
            'status': 'error',
            'error': str(e),
            'command_id': os.path.basename(command_file).replace('command_', '').replace('.json', '')
        }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π inference –¥–µ–º–æ–Ω...")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
    os.makedirs("inference_commands", exist_ok=True)
    os.makedirs("task_results", exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑
    if not load_models_once():
        logger.error("üíÄ –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É")
        return
    
    # –°—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
    create_ready_flag()
    logger.info("üèÅ –î–µ–º–æ–Ω –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    logger.info("üìÅ –û–∂–∏–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—ã –≤ –ø–∞–ø–∫–µ inference_commands/")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
    while True:
        try:
            # –ò—â–µ–º –Ω–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã
            command_files = glob.glob("inference_commands/command_*.json")
            
            for command_file in command_files:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–∞–Ω–¥—É
                result = process_command_file(command_file)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result_file = command_file.replace('command_', 'result_')
                with open(result_file, 'w') as f:
                    json.dump(result, f)
                
                # –£–¥–∞–ª—è–µ–º –∫–æ–º–∞–Ω–¥—É
                os.remove(command_file)
                
                # –û—á–∏—â–∞–µ–º GPU –∫–µ—à –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ü–∏—è–º–∏
                clear_gpu_cache()
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
            time.sleep(1)
            
        except KeyboardInterrupt:
            logger.info("\nüõë –î–µ–º–æ–Ω –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
            time.sleep(5)

if __name__ == "__main__":
    main() 