import os
import io
import base64
import uuid
from PIL import Image

import torch
from celery import Celery
from diffusers import LTXConditionPipeline, LTXLatentUpsamplePipeline
from diffusers.pipelines.ltx.pipeline_ltx_condition import LTXVideoCondition
from diffusers.utils import export_to_video, load_image, load_video
from my_celery import celery_app, get_models, load_models_on_startup

def round_to_nearest_resolution_acceptable_by_vae(height, width, pipe):
    """–û–∫—Ä—É–≥–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä—ã –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö, –ø—Ä–∏–µ–º–ª–µ–º—ã—Ö –¥–ª—è VAE."""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è VAE
    spatial_compression_ratio = pipe.vae.spatial_compression_ratio
    height = height - (height % spatial_compression_ratio)
    width = width - (width % spatial_compression_ratio)
    return height, width

@celery_app.task(name="celery_task.generate_video_task")
def generate_video_task(
    prompt, 
    negative_prompt=None, 
    image_base64=None, 
    width=720, 
    height=1280, 
    num_frames=121, 
    seed=0,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    num_inference_steps=50,
    denoise_strength=0.3,
    decode_timestep=0.05,
    image_cond_noise_scale=0.025,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã downscale/upscale
    downscale_factor=0.67,
    upscale_factor=2.0,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–µ–Ω–æ–π–∑–∞
    final_num_inference_steps=10,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã VAE
    vae_spatial_compression_ratio=32,
    vae_temporal_compression_ratio=8,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Transformer
    transformer_num_attention_heads=32,
    transformer_num_layers=48,
    transformer_attention_head_dim=128,
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Scheduler
    scheduler_num_train_timesteps=1000,
    scheduler_stochastic_sampling=False,
    scheduler_use_karras_sigmas=False,
):
    print(f"üîç –ü–æ–ª—É—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"üîç final_num_inference_steps: {final_num_inference_steps}")
    print(f"üîç num_inference_steps: {num_inference_steps}")
    print(f"üîç downscale_factor: {downscale_factor}")
    print(f"üîç upscale_factor: {upscale_factor}")
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤
    width = int(width)
    height = int(height)
    num_frames = int(num_frames)
    num_inference_steps = int(num_inference_steps)
    final_num_inference_steps = int(final_num_inference_steps)
    seed = int(seed)
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è VAE –∏ Transformer
    vae_spatial_compression_ratio = int(vae_spatial_compression_ratio)
    vae_temporal_compression_ratio = int(vae_temporal_compression_ratio)
    transformer_num_attention_heads = int(transformer_num_attention_heads)
    transformer_num_layers = int(transformer_num_layers)
    transformer_attention_head_dim = int(transformer_attention_head_dim)
    scheduler_num_train_timesteps = int(scheduler_num_train_timesteps)

    # –ì—Ä—É–∑–∏–º –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç (—Ç–æ–ª—å–∫–æ –≤ –≤–æ—Ä–∫–µ—Ä–µ)
    load_models_on_startup()
    pipe, pipe_upsample = get_models()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ VAE –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö
    if hasattr(pipe, 'vae') and hasattr(pipe.vae, 'spatial_compression_ratio'):
        if pipe.vae.spatial_compression_ratio != vae_spatial_compression_ratio:
            pipe.vae.spatial_compression_ratio = vae_spatial_compression_ratio
        if hasattr(pipe.vae, 'temporal_compression_ratio') and pipe.vae.temporal_compression_ratio != vae_temporal_compression_ratio:
            pipe.vae.temporal_compression_ratio = vae_temporal_compression_ratio

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Transformer –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö
    if hasattr(pipe, 'transformer'):
        if hasattr(pipe.transformer, 'config') and hasattr(pipe.transformer.config, 'num_attention_heads') and pipe.transformer.config.num_attention_heads != transformer_num_attention_heads:
            pipe.transformer.config.num_attention_heads = transformer_num_attention_heads
        if hasattr(pipe.transformer, 'config') and hasattr(pipe.transformer.config, 'num_layers') and pipe.transformer.config.num_layers != transformer_num_layers:
            pipe.transformer.config.num_layers = transformer_num_layers
        if hasattr(pipe.transformer, 'config') and hasattr(pipe.transformer.config, 'attention_head_dim') and pipe.transformer.config.attention_head_dim != transformer_attention_head_dim:
            pipe.transformer.config.attention_head_dim = transformer_attention_head_dim

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Scheduler
    if hasattr(pipe, 'scheduler'):
        if hasattr(pipe.scheduler, 'config') and hasattr(pipe.scheduler.config, 'num_train_timesteps') and pipe.scheduler.config.num_train_timesteps != scheduler_num_train_timesteps:
            pipe.scheduler.config.num_train_timesteps = scheduler_num_train_timesteps
        if hasattr(pipe.scheduler, 'config') and hasattr(pipe.scheduler.config, 'stochastic_sampling') and pipe.scheduler.config.stochastic_sampling != scheduler_stochastic_sampling:
            pipe.scheduler.config.stochastic_sampling = scheduler_stochastic_sampling
        if hasattr(pipe.scheduler, 'config') and hasattr(pipe.scheduler.config, 'use_karras_sigmas') and pipe.scheduler.config.use_karras_sigmas != scheduler_use_karras_sigmas:
            pipe.scheduler.config.use_karras_sigmas = scheduler_use_karras_sigmas

    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Å —É—á–µ—Ç–æ–º downscale_factor –∏ –æ–∫—Ä—É–≥–ª—è–µ–º –¥–æ –∫—Ä–∞—Ç–Ω—ã—Ö 32
    print(f"üîç –ò—Å—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: {width}x{height}")
    print(f"üîç Downscale factor: {downscale_factor}")
    
    downscaled_height = int(height * downscale_factor)
    downscaled_width = int(width * downscale_factor)
    print(f"üîç –ü–æ—Å–ª–µ downscale: {downscaled_width}x{downscaled_height}")
    
    # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —á–∏—Å–ª–∞, –∫—Ä–∞—Ç–Ω–æ–≥–æ 32
    downscaled_height = (downscaled_height // 32) * 32
    downscaled_width = (downscaled_width // 32) * 32
    print(f"üîç –ü–æ—Å–ª–µ –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è –¥–æ 32: {downscaled_width}x{downscaled_height}")
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã –Ω–µ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö
    downscaled_height = max(32, downscaled_height)
    downscaled_width = max(32, downscaled_width)
    print(f"üîç –ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–∏–Ω–∏–º—É–º–∞: {downscaled_width}x{downscaled_height}")
    
    downscaled_height, downscaled_width = round_to_nearest_resolution_acceptable_by_vae(downscaled_height, downscaled_width, pipe)
    print(f"üîç –ü–æ—Å–ª–µ VAE –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è: {downscaled_width}x{downscaled_height}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º: text-to-video –∏–ª–∏ image-to-video
    if image_base64 and isinstance(image_base64, str):
        # image-to-video
        image = Image.open(io.BytesIO(base64.b64decode(image_base64))).convert("RGB")
        image = image.resize((width, height), Image.Resampling.LANCZOS)
        # –°–∂–∏–º–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ –≤–∏–¥–µ–æ (1 –∫–∞–¥—Ä)
        temp_video_path = f"temp_{uuid.uuid4().hex}.mp4"
        export_to_video([image], temp_video_path, fps=30)
        video_frames = load_video(temp_video_path)
        os.remove(temp_video_path)
        condition1 = LTXVideoCondition(video=video_frames, frame_index=0)
        conditions = [condition1]
    else:
        # text-to-video
        conditions = None

    # Part 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ –≤ –Ω–∏–∑–∫–æ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏
    print(f"üîç –ü–µ—Ä–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏: {downscaled_width}x{downscaled_height}")
    latents = pipe(
        conditions=conditions,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=downscaled_width,
        height=downscaled_height,
        num_frames=num_frames,
        num_inference_steps=num_inference_steps,
        generator=torch.Generator("cuda").manual_seed(seed),
        output_type="latent",
    ).frames
    print(f"üîç –†–∞–∑–º–µ—Ä—ã –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö latents: {latents.shape}")

    # Part 2. –ê–ø—Å–∫–µ–π–ª –ª–∞—Ç–µ–Ω—Ç–æ–≤
    upscaled_height = int(downscaled_height * upscale_factor)
    upscaled_width = int(downscaled_width * upscale_factor)
    
    # –û–∫—Ä—É–≥–ª—è–µ–º –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —á–∏—Å–ª–∞, –∫—Ä–∞—Ç–Ω–æ–≥–æ 32
    upscaled_height = (upscaled_height // 32) * 32
    upscaled_width = (upscaled_width // 32) * 32
    
    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã –Ω–µ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö
    upscaled_height = max(32, upscaled_height)
    upscaled_width = max(32, upscaled_width)
    upscaled_latents = pipe_upsample(
        latents=latents,
        output_type="latent"
    ).frames

    # Part 3. –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–µ–Ω–æ–π–∑ –∏ –¥–µ–∫–æ–¥
    print(f"üîç –†–∞–∑–º–µ—Ä—ã upscaled_latents: {upscaled_latents.shape}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –ª–∞—Ç–µ–Ω—Ç–æ–≤
    latent_height, latent_width = upscaled_latents.shape[-2], upscaled_latents.shape[-1]
    final_width = latent_width * 32
    final_height = latent_height * 32
    print(f"üîç –í—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {final_width}x{final_height}")
    print(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º final_num_inference_steps: {final_num_inference_steps}")
    
    video = pipe(
        conditions=conditions,
        prompt=prompt,
        negative_prompt=negative_prompt,
        width=final_width,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ª–∞—Ç–µ–Ω—Ç–∞–º
        height=final_height,
        num_frames=num_frames,
        denoise_strength=denoise_strength,
        num_inference_steps=final_num_inference_steps,
        latents=upscaled_latents,
        decode_timestep=decode_timestep,
        image_cond_noise_scale=image_cond_noise_scale,
        generator=torch.Generator("cuda").manual_seed(seed),
        output_type="pil",
    ).frames[0]

    # Part 4. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é
    video = [frame.resize((width, height)) for frame in video]

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥–µ–æ
    output_path = f"result_{uuid.uuid4().hex}.mp4"
    export_to_video(video, output_path, fps=30)
    return output_path