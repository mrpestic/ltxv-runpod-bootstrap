#!/usr/bin/env python3
import os
import sys
import base64
import io
import uuid
from typing import Any, Dict

import runpod


# –ë–∞–∑–æ–≤—ã–µ –ø—É—Ç–∏ –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫–µ—à–µ–π –∏ —Ä–∞–±–æ—á–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
WORKSPACE_DIR = "/workspace"
LTX_DIR = os.path.join(WORKSPACE_DIR, "LTX-Video")
HF_CACHE = os.path.join(WORKSPACE_DIR, ".cache", "huggingface")


def _ensure_env():
    os.makedirs(HF_CACHE, exist_ok=True)
    os.environ.setdefault("HF_HOME", HF_CACHE)
    os.environ.setdefault("HUGGINGFACE_HUB_CACHE", HF_CACHE)
    os.environ.setdefault("TRANSFORMERS_CACHE", HF_CACHE)
    os.environ.setdefault("DIFFUSERS_CACHE", HF_CACHE)
    os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0")


def _prepare_imports():
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –º–æ–∂–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –¥–µ–º–æ–Ω–∞ –∏ LTX-Video
    if LTX_DIR not in sys.path:
        sys.path.insert(0, LTX_DIR)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ pipeline/–∫–æ–Ω—Ñ–∏–≥ –¥–µ–º–æ–Ω–∞
global_pipeline = None
global_pipeline_config = None


def init():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ (cold start): –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –æ–¥–∏–Ω —Ä–∞–∑."""
    global global_pipeline, global_pipeline_config
    
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    _ensure_env()
    _prepare_imports()

    # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ LTX-Video
    os.chdir(LTX_DIR)
    logger.info(f"üîß –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")

    # –í–µ—Å–∞ —É–∂–µ –∑–∞–ø–µ—á–µ–Ω—ã –≤ –æ–±—Ä–∞–∑ –Ω–∞ —ç—Ç–∞–ø–µ —Å–±–æ—Ä–∫–∏ Docker, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö –Ω–∞–ª–∏—á–∏–µ
    ckpt_candidates = [
        os.path.join(LTX_DIR, "models", "ltxv-13b-0.9.8-distilled.safetensors"),
        os.path.join(LTX_DIR, "models", "ltxv-13b-0.9.8-distilled", "model.safetensors"),
    ]
    
    logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤...")
    for candidate in ckpt_candidates:
        logger.info(f"  - {candidate}: {'‚úÖ –Ω–∞–π–¥–µ–Ω' if os.path.exists(candidate) else '‚ùå –Ω–µ –Ω–∞–π–¥–µ–Ω'}")
    
    if not any(os.path.exists(p) for p in ckpt_candidates):
        raise RuntimeError(f"–í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: {ckpt_candidates}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ñ–∏–≥–∞
    config_path = os.path.join(LTX_DIR, "ltxv-13b-0.9.8-distilled.yaml")
    if not os.path.exists(config_path):
        raise RuntimeError(f"–ö–æ–Ω—Ñ–∏–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
    logger.info(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –Ω–∞–π–¥–µ–Ω: {config_path}")

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –¥–µ–º–æ–Ω–∞ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
    import importlib
    try:
        daemon = importlib.import_module('inference_daemon_official')
        logger.info("‚úÖ –ú–æ–¥—É–ª—å inference_daemon_official –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ inference_daemon_official: {e}")

    logger.info("üé¨ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏...")
    try:
        ok = daemon.load_models_once()
        if not ok:
            raise RuntimeError("load_models_once() –≤–µ—Ä–Ω—É–ª False - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ")
    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")

    # –°—Ç–∞–≤–∏—Ç —Ñ–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Å—Ç–∞—Ä—Ç–∞–ø–µ)
    daemon.create_ready_flag()
    logger.info("‚úÖ –§–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω")

    # –ü—Ä–æ–∫–∏–Ω–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    global_pipeline = daemon.global_pipeline
    global_pipeline_config = daemon.global_pipeline_config
    
    if global_pipeline is None:
        raise RuntimeError("global_pipeline is None –ø–æ—Å–ª–µ load_models_once()")
    if global_pipeline_config is None:
        raise RuntimeError("global_pipeline_config is None –ø–æ—Å–ª–µ load_models_once()")
    
    logger.info("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")


def _decode_image_to_file(image_base64: str) -> str:
    from PIL import Image
    image_bytes = base64.b64decode(image_base64.split(',')[1] if ',' in image_base64 else image_base64)
    image = Image.open(io.BytesIO(image_bytes))
    if image.mode != 'RGB':
        image = image.convert('RGB')
    filename = f"temp_image_{uuid.uuid4().hex}.jpg"
    image.save(filename)
    return filename


def handler(event: Dict[str, Any]) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–¥–∞—á–∏ RunPod Serverless.

    –û–∂–∏–¥–∞–µ–º—ã–π payload –≤ event["input"]:
    - prompt (str, required)
    - negative_prompt (str, optional)
    - width (int, optional)
    - height (int, optional)
    - num_frames (int, optional)
    - seed (int, optional)
    - image_base64 (str, optional) ‚Äî –¥–ª—è image-to-video
    """
    from ltx_video.inference import InferenceConfig
    from inference_daemon_official import infer_with_ready_pipeline

    if "input" not in event:
        return {"status": "ERROR", "error": "payload.input is missing"}

    data = event["input"] or {}

    # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ init –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è (–∏–ª–∏ –≥–ª–æ–±–∞–ª–∏ –ø—É—Å—Ç—ã–µ) ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—É—Ç
    global global_pipeline, global_pipeline_config
    if global_pipeline is None or global_pipeline_config is None:
        init()

    prompt = data.get("prompt")
    if not prompt:
        return {"status": "ERROR", "error": "prompt is required"}

    negative_prompt = data.get("negative_prompt", "worst quality, inconsistent motion, blurry, jittery, distorted")
    width = int(data.get("width", 1280))
    height = int(data.get("height", 720))
    num_frames = int(data.get("num_frames", 120))
    seed = int(data.get("seed", 42))
    image_b64 = data.get("image_base64")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ image-to-video, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    conditioning_media_paths = None
    tmp_image_path = None
    if image_b64:
        try:
            tmp_image_path = _decode_image_to_file(image_b64)
            conditioning_media_paths = [tmp_image_path]
        except Exception as e:
            return {"status": "ERROR", "error": f"failed to decode image: {e}"}

    # –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    config = InferenceConfig(
        prompt=prompt,
        negative_prompt=negative_prompt,
        height=height,
        width=width,
        num_frames=num_frames,
        seed=seed,
        pipeline_config="ltxv-13b-0.9.8-distilled.yaml",
        frame_rate=24,
    )

    if conditioning_media_paths:
        config.conditioning_media_paths = conditioning_media_paths
        config.conditioning_start_frames = [0]

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–µ–º–æ–Ω–∞ —Å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º pipeline
    try:
        result_paths = infer_with_ready_pipeline(config, global_pipeline, global_pipeline_config)
    except Exception as e:
        # –£–±–æ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if tmp_image_path and os.path.exists(tmp_image_path):
            try:
                os.remove(tmp_image_path)
            except Exception:
                pass
        return {"status": "ERROR", "error": str(e)}

    # –£–±–æ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    if tmp_image_path and os.path.exists(tmp_image_path):
        try:
            os.remove(tmp_image_path)
        except Exception:
            pass

    if not result_paths:
        return {"status": "ERROR", "error": "no output produced"}

    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ RunPod –∏ –≤–µ—Ä–Ω—É—Ç—å –ø—É–±–ª–∏—á–Ω—ã–π URL
    result_url = None
    try:
        from runpod.serverless.utils import rp_upload
        upload_result = rp_upload.upload_file_to_bucket(result_paths[0])
        if isinstance(upload_result, str):
            result_url = upload_result
        elif isinstance(upload_result, dict):
            result_url = upload_result.get("url") or upload_result.get("file_url")
    except Exception:
        result_url = None

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ –ü–ï–†–ï–î –∑–∞–≥—Ä—É–∑–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ base64
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            import gc
            gc.collect()
    except Exception:
        pass
    
    # –ï—Å–ª–∏ S3 –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–∏–¥–µ–æ –∫–∞–∫ base64
    video_base64 = None
    if result_url is None:
        try:
            with open(result_paths[0], 'rb') as f:
                video_bytes = f.read()
                video_base64 = base64.b64encode(video_bytes).decode('utf-8')
        except Exception as e:
            pass  # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω–µ–º null
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å, URL (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏ base64 (–µ—Å–ª–∏ –Ω–µ—Ç URL)
    return {
        "status": "SUCCESS",
        "result_path": result_paths[0],
        "result_url": result_url,
        "video_base64": video_base64,
        "all_results": result_paths,
    }


runpod.serverless.start({
    "handler": handler,
    "init": init,
})


